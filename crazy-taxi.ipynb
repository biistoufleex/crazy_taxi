{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a973eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2663d",
   "metadata": {},
   "source": [
    "# Method d'entrainement de Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "752753c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Qlearning(env=gym.make(\"Taxi-v3\"),\n",
    "          episodes: int = 25000,\n",
    "          lr: float = 0.01,\n",
    "          gamma: float = 0.99,\n",
    "          epsilon: float = 1,\n",
    "          max_epsilon: float = 1,\n",
    "          min_epsilon: float = 0.001,\n",
    "          epsilon_decay: float = 0.01,\n",
    "          path_table: str = \"qtable\") -> tuple[float, int]:\n",
    "    \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_reward = []\n",
    "    steps_per_episode = []\n",
    "    epsilon_vec = []\n",
    "    success_rate = []\n",
    "    success_count = 0\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.item() if isinstance(state, np.ndarray) else state\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay * e)\n",
    "        epsilon_vec.append(epsilon)\n",
    "\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = next_state.item() if isinstance(next_state, np.ndarray) else next_state\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            current_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            q_table[state, action] = (1 - lr) * current_value + lr * (reward + gamma * next_max)\n",
    "            state = next_state\n",
    "\n",
    "        total_reward.append(episode_reward)\n",
    "        steps_per_episode.append(episode_steps)\n",
    "\n",
    "        if episode_reward > 0:\n",
    "            success_count += 1\n",
    "        success_rate.append(success_count / (e + 1))\n",
    "\n",
    "    execution_time = (time.time() - start_time)\n",
    "\n",
    "    np.save(path_table, q_table)\n",
    "\n",
    "    return np.round(execution_time, 2), np.mean(total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5035dbb",
   "metadata": {},
   "source": [
    "# Methode pour trouver les meilleurs parametres de Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1076379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.001, gamma=0.9, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.9, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.95, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.001, gamma=0.99, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.9, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.95, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.01, gamma=0.99, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.9, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.95, epsilon=0.3, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.1, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.1, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.1, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.2, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.2, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.2, epsilon_decay=0.1\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.3, epsilon_decay=0.001\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.3, epsilon_decay=0.01\n",
      "Training with lr=0.1, gamma=0.99, epsilon=0.3, epsilon_decay=0.1\n",
      "Best parameters: lr=0.1, gamma=0.99, epsilon=0.3, epsilon_decay=0.1\n",
      "Best mean reward: -98.766\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search():\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    gammas = [0.9, 0.95, 0.99]\n",
    "    epsilons = [0.1, 0.2, 0.3]\n",
    "    epsilon_decays = [0.001, 0.01, 0.1]\n",
    "\n",
    "    best_params = None\n",
    "    best_reward = float('-inf')\n",
    "\n",
    "    for lr, gamma, epsilon, epsilon_decay in itertools.product(learning_rates, gammas, epsilons, epsilon_decays):\n",
    "        print(f\"Training with lr={lr}, gamma={gamma}, epsilon={epsilon}, epsilon_decay={epsilon_decay}\")\n",
    "        _, mean_reward = train_Qlearning(env=gym.make(\"Taxi-v3\"),\n",
    "                               episodes=1000,\n",
    "                               lr=lr,\n",
    "                               gamma=gamma,\n",
    "                               epsilon=epsilon,\n",
    "                               epsilon_decay=epsilon_decay)\n",
    "        \n",
    "        if mean_reward > best_reward:\n",
    "            best_reward = mean_reward\n",
    "            best_params = (lr, gamma, epsilon, epsilon_decay)\n",
    "    \n",
    "    print(f\"Best parameters: lr={best_params[0]}, gamma={best_params[1]}, epsilon={best_params[2]}, epsilon_decay={best_params[3]}\")\n",
    "    print(f\"Best mean reward: {best_reward}\")\n",
    "\n",
    "grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883872e",
   "metadata": {},
   "source": [
    "## Meilleur parametres pour Q-learning\n",
    "Best parameters: lr=0.1, gamma=0.99, epsilon=0.3, epsilon_decay=0.1\n",
    "\n",
    "Best mean reward: -98.766"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6803fe",
   "metadata": {},
   "source": [
    "# Methode d'entrainement de Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55450ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env=gym.make(\"Taxi-v3\"),\n",
    "          episodes=2000,\n",
    "          gamma=0.95,\n",
    "          epsilon=1,\n",
    "          max_epsilon=1,\n",
    "          min_epsilon=0.001,\n",
    "          epsilon_decay=0.01,\n",
    "          alpha=0.85,\n",
    "          path_table: str = \"sarsa_qtable\"):\n",
    "    \n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_reward = []\n",
    "    steps_per_episode = []\n",
    "    epsilon_vec = []\n",
    "    success_rate = []\n",
    "    success_count = 0\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.item() if isinstance(state, np.ndarray) else state\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay * e)\n",
    "        epsilon_vec.append(epsilon)\n",
    "\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = next_state.item() if isinstance(next_state, np.ndarray) else next_state\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = np.argmax(Q[next_state, :])\n",
    "\n",
    "            predict = Q[state, action]\n",
    "            target = reward + gamma * Q[next_state, next_action]\n",
    "            Q[state, action] = Q[state, action] + alpha * (target - predict)\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        total_reward.append(episode_reward)\n",
    "        steps_per_episode.append(episode_steps)\n",
    "\n",
    "        if episode_reward > 0:\n",
    "            success_count += 1\n",
    "        success_rate.append(success_count / (e + 1))\n",
    "\n",
    "    execution_time = (time.time() - start_time)\n",
    "\n",
    "    return np.round(execution_time, 2), np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f8019",
   "metadata": {},
   "source": [
    "# Methode pour trouver les meilleurs parametres de Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fce02de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations: 100%|██████████| 27/27 [02:07<00:00,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: gamma=0.9, epsilon=0.1, alpha=0.5\n",
      "Best mean reward: -11.929333333333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_best_params_sarsa(env_name=\"Taxi-v3\", num_episodes=2000, num_runs=3):\n",
    "    gamma_range = [0.9, 0.95, 0.99]\n",
    "    epsilon_decay_range = [0.001, 0.01, 0.1]\n",
    "    alpha_range = [0.1, 0.5, 0.9]\n",
    "\n",
    "    best_params = None\n",
    "    best_mean_reward = float('-inf')\n",
    "\n",
    "    param_combinations = list(itertools.product(gamma_range, epsilon_decay_range, alpha_range))\n",
    "\n",
    "    for gamma, epsilon_decay, alpha in tqdm(param_combinations, desc=\"Testing parameter combinations\"):\n",
    "        total_mean_reward = 0\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            env = gym.make(env_name)\n",
    "            _, mean_reward = train_sarsa(\n",
    "                env=env,\n",
    "                episodes=num_episodes,\n",
    "                gamma=gamma,\n",
    "                epsilon_decay=epsilon_decay,\n",
    "                alpha=alpha\n",
    "            )\n",
    "            total_mean_reward += mean_reward\n",
    "\n",
    "        avg_mean_reward = total_mean_reward / num_runs\n",
    "\n",
    "        if avg_mean_reward > best_mean_reward:\n",
    "            best_mean_reward = avg_mean_reward\n",
    "            best_params = (gamma, epsilon_decay, alpha)\n",
    "\n",
    "    print(f\"Best parameters: gamma={best_params[0]}, epsilon={best_params[1]}, alpha={best_params[2]}\")\n",
    "    print(f\"Best mean reward: {best_reward}\")\n",
    "\n",
    "    return best_params, best_mean_reward\n",
    "\n",
    "best_params, best_reward = find_best_params_sarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f80e47",
   "metadata": {},
   "source": [
    "## Meilleur parametres pour SARSA\n",
    "Best parameters: gamma=0.9, epsilon=0.1, alpha=0.5\n",
    "\n",
    "Best mean reward: -11.929333333333332\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c63c784",
   "metadata": {},
   "source": [
    "# Comparaison graph entrainement Q-learning et Sarsa\n",
    "\n",
    "### Q-learning\n",
    "![Q-Learning graph](q-learning/QLearning_graph.png)\n",
    "\n",
    "### Sarsa\n",
    "![Sarsa graph](sarsa/sarsa_graph.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3baa2b8",
   "metadata": {},
   "source": [
    "# Analyse des graphiques de Q-learning et SARSA\n",
    "\n",
    "***Graphique 1:*** Q-learning\n",
    "\n",
    "***Graphique 2:*** SARSA\n",
    "\n",
    "En analysant ces deux graphiques, nous pouvons faire les déductions suivantes:\n",
    "\n",
    "### Convergence:\n",
    "Le graphique 1 montre une convergence beaucoup plus stable et rapide que le graphique 2. Les courbes de récompenses (vert) et d'étapes (orange) se stabilisent rapidement dans le graphique 1, tandis qu'elles restent très volatiles dans le graphique 2.\n",
    "\n",
    "### Stabilité:\n",
    "Le premier algorithme (graphique 1) semble beaucoup plus stable. Les fluctuations dans les récompenses et les étapes diminuent considérablement au fil du temps. En revanche, le deuxième algorithme (graphique 2) montre des variations importantes tout au long de l'entraînement.\n",
    "\n",
    "### Performance:\n",
    "L'algorithme du graphique 1 semble atteindre de meilleures performances moyennes. Les récompenses se stabilisent à un niveau plus élevé et les étapes à un niveau plus bas, ce qui est généralement souhaitable dans l'apprentissage par renforcement.\n",
    "\n",
    "### Exploration (Epsilon):\n",
    "Dans les deux cas, l'epsilon (ligne rouge) décroît de manière similaire, mais son impact sur la performance semble très différent. L'algorithme 1 semble mieux exploiter la réduction de l'exploration.\n",
    "\n",
    "### Apprentissage:\n",
    "L'algorithme 1 semble apprendre plus efficacement. Il montre une amélioration constante et maintient ses performances, tandis que l'algorithme 2 semble avoir du mal à maintenir des performances stables.\n",
    "\n",
    "### Robustesse:\n",
    "L'algorithme 1 paraît plus robuste aux variations de l'environnement ou aux aléas de l'apprentissage, tandis que l'algorithme 2 semble très sensible à ces facteurs.\n",
    "\n",
    "### Conclusion:\n",
    "En conclusion, l'algorithme Q-learning semble nettement supérieur en termes de stabilité, de convergence et de performance globale pour cette tâche particulière. Malgré le fait que SARSA ait une meilleure récompense moyenne, Q-learning démontre une plus grande stabilité et une convergence plus rapide, ce qui en fait un choix préférable pour cette application.mble nettement supérieur en termes de stabilité, de convergence et de performance globale pour cette tâche particulière.\n",
    "\n",
    "### Données complémentaires:\n",
    "**SARSA:**\n",
    "- Temps d'entraînement: 12.33s (0.21min, 0.0h)\n",
    "- Temps moyen par épisode: 0.000123s\n",
    "- Temps d'exécution: 12.33 secondes\n",
    "- Récompense moyenne: 7.09538\n",
    "\n",
    "**Q-learning:**\n",
    "- Temps d'entraînement: 27.09s (0.45min, 0.01h)\n",
    "- Temps moyen par épisode: 0.000271s\n",
    "- Temps d'exécution: 27.09 secondes\n",
    "- Récompense moyenne: -1.69379"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f5c5c",
   "metadata": {},
   "source": [
    "# Graphiques complementaire\n",
    "\n",
    "## Brute force\n",
    "![bruteForce_reward_distribution](brute-force/bruteforce_reward_distribution.png)\n",
    "\n",
    "![bruteForce_reward_and_steps](brute-force/bruteforce_rewards_and_steps.png)\n",
    "\n",
    "![bruteforce_steps_distribution](brute-force/bruteforce_steps_distribution.png)\n",
    "\n",
    "![bruteforce_success_rate](brute-force/bruteforce_success_rate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51534982",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "\n",
    "![monte_carlo_graph.png](monte-carlo/monte_carlo_graph.png)\n",
    "![monte_carlo_reward_distribution.png](monte-carlo/monte_carlo_reward_distribution.png)\n",
    "![monte_carlo_rewards_and_steps.png](monte-carlo/monte_carlo_rewards_and_steps.png)\n",
    "![monte_carlo_step_distribution.png](monte-carlo/monte_carlo_steps_distribution.png)\n",
    "![monte_carlo_success_rate.png](monte-carlo/monte_carlo_success_rate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0f8ad",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "![QLearning_graph.png](q-learning/QLearning_graph.png)\n",
    "![QLearning_reward_distribution.png](q-learning/QLearning_reward_distribution.png)\n",
    "![QLearning_rewards_and_steps.png](q-learning/QLearning_rewards_and_steps.png)\n",
    "![QLearning_step_distribution.png](q-learning/monte_carlo_steps_distribution.png)\n",
    "![QLearning_success_rate.png](q-learning/QLearning_success_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929db17",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "![SARSA_graph.png](sarsa/sarsa_graph.png)\n",
    "![SARSA_reward_distribution.png](sarsa/sarsa_reward_distribution.png)\n",
    "![SARSA_rewards_and_steps.png](sarsa/sarsa_rewards_and_steps.png)\n",
    "![SARSA_step_distribution.png](sarsa/sarsa_steps_distribution.png)\n",
    "![SARSA_success_rate.png](sarsa/sarsa_success_rate.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
