{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e57976-c79f-4812-a519-e3543687423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in ./.venv/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.10/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.venv/lib/python3.10/site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.10/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: pygame in ./.venv/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: progressbar in ./.venv/lib/python3.10/site-packages (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install -q matplotlib\n",
    "!pip install pygame\n",
    "!pip install numpy\n",
    "!pip install progressbar\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a44e00-1670-453a-8141-5e51ab39ba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "# env = gym.make('Taxi-v3', render_mode=\"human\")\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.6  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 100000\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97617d14-b802-4f83-ad19-d37d60808027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bar = progressbar.ProgressBar()\n",
    "bar(range(num_episodes))\n",
    "bar.start()\n",
    "\n",
    "# Training the agent\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()[0]  # Get the initial state from the reset method\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = next_state  # Ensure next_state is an integer\n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        # Q-learning formula\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    bar.update(i)\n",
    "\n",
    "bar.finish()\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87e8665a-5534-44b6-bddc-6f1db6974973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 10000 episodes:\n",
      "Average timesteps per episode: 13.0658\n",
      "Average penalties per episode: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bar = progressbar.ProgressBar()\n",
    "bar(range(num_episodes))\n",
    "bar.start()\n",
    "\n",
    "# Evaluate the agent's performance after Q-learning\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 10000\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = next_state\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    \n",
    "    bar.update(episode)\n",
    "\n",
    "bar.finish()\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd093532-3052-4831-9794-42863f43aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time steps: 13, Penalties: 0\n",
      "Time steps: 17, Penalties: 0\n",
      "Time steps: 18, Penalties: 0\n",
      "Time steps: 18, Penalties: 0\n",
      "Time steps: 11, Penalties: 0\n",
      "Time steps: 18, Penalties: 0\n",
      "Time steps: 11, Penalties: 0\n",
      "Time steps: 8, Penalties: 0\n",
      "Time steps: 14, Penalties: 0\n",
      "Time steps: 13, Penalties: 0\n"
     ]
    }
   ],
   "source": [
    "# Visualize the agent 10 laps\n",
    "env = gym.make('Taxi-v3', render_mode=\"human\")\n",
    "\n",
    "for _ in range(10):\n",
    "    state = env.reset()[0]\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = next_state\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "    print(f\"Time steps: {epochs}, Penalties: {penalties}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
